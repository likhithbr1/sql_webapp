import json, re, numpy as np, faiss
from sqlalchemy import create_engine, text
from sentence_transformers import SentenceTransformer
from langchain_community.utilities import SQLDatabase
from langchain_community.llms import Ollama
from langchain.agents.agent_types import AgentType
from langchain_community.agent_toolkits.sql.base import create_sql_agent
from langchain_core.runnables import RunnableLambda, RunnablePassthrough

# ------------------------------
# Config
# ------------------------------
INDEX_PATH = "schema_index/faiss_index.bin"
META_PATH = "schema_index/table_metadata.json"
EMBED_MODEL_NAME = "BAAI/bge-small-en"
TOP_K = 3
DB_URI = "sqlite:///chatbot.db"
OLLAMA_MODEL = "llama3"  # or llama2:13b-chat

# ------------------------------
# FAISS + Schema Utilities
# ------------------------------
def load_faiss_and_metadata(index_path, meta_path):
    index = faiss.read_index(index_path)
    with open(meta_path, "r") as f:
        meta = json.load(f)
    return index, meta

def build_reverse_fk_map(metadata):
    rev_map = {m["table_name"]: set() for m in metadata.values()}
    fk_pattern = re.compile(r"REFERENCES\s+?(\w+)?", re.IGNORECASE)
    for m in metadata.values():
        for ref in fk_pattern.findall(m["create_stmt"]):
            if ref in rev_map:
                rev_map[ref].add(m["table_name"])
    return rev_map

def parse_forward_fks(ddl: str):
    return set(re.findall(r"REFERENCES\s+?(\w+)?", ddl, re.IGNORECASE))

def semantic_search(query, embed_model, faiss_index, top_k):
    q_emb = embed_model.encode(query)
    q_emb = np.array([q_emb], dtype="float32")
    _, I = faiss_index.search(q_emb, top_k)
    return I[0]

def expand_with_related(idx_list, metadata, rev_fk_map):
    tables = {metadata[str(i)]["table_name"] for i in idx_list}
    extra = set()
    for i in idx_list:
        ddl = metadata[str(i)]["create_stmt"]
        table = metadata[str(i)]["table_name"]
        extra.update(parse_forward_fks(ddl))
        extra.update(rev_fk_map.get(table, set()))
    return tables.union(extra)

def build_schema_snippet(table_names, metadata):
    return "\n\n".join(
        m["create_stmt"] for m in metadata.values()
        if m["table_name"] in table_names
    )

# ------------------------------
# Initialization
# ------------------------------
def init_models():
    global _faiss_index, _metadata, _rev_fk_map, _embed_model
    _faiss_index, _metadata = load_faiss_and_metadata(INDEX_PATH, META_PATH)
    _rev_fk_map = build_reverse_fk_map(_metadata)
    _embed_model = SentenceTransformer(EMBED_MODEL_NAME)

# ------------------------------
# LangChain SQL Agent
# ------------------------------
def custom_schema_chain(input_dict):
    question = input_dict["question"]
    idxs = semantic_search(question, _embed_model, _faiss_index, TOP_K)
    tables = expand_with_related(idxs, _metadata, _rev_fk_map)
    schema = build_schema_snippet(tables, _metadata)
    return {"table_names_to_use": schema}

def build_agent_chain():
    db = SQLDatabase.from_uri(DB_URI)
    llm = Ollama(model=OLLAMA_MODEL)

    query_chain = create_sql_agent(
        llm=llm,
        db=db,
        agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
        verbose=True,
        return_intermediate_steps=True,
        agent_executor_kwargs={"handle_parsing_errors": True}
    )

    input_mapper = RunnableLambda(lambda x: {"input": x["question"]} if "question" in x and "input" not in x else x)
    table_chain = RunnableLambda(custom_schema_chain)
    full_chain = input_mapper | RunnablePassthrough.assign(table_names_to_use=table_chain) | query_chain
    return full_chain

def process_question_agentic(question: str) -> dict:
    try:
        agent_chain = build_agent_chain()
        result = agent_chain.invoke({"input": question, "question": question})
        return {
            "answer": result.get("output", "No answer generated.")
        }
    except Exception as e:
        return {"answer": None, "error": str(e)}
